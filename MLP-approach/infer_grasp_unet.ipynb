{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f70e1a49",
   "metadata": {},
   "source": [
    "figure = 66.66\n",
    "bottle = 62.96\n",
    "pen = 54.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e754fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\49174\\anaconda3\\envs\\praktikum\\Lib\\site-packages\")\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataset_jacquard_samples import JacquardSamples\n",
    "from utils import get_transform, get_inv_transform\n",
    "from bce_model import BCEGraspTransformer\n",
    "from utils_train import create_correct_false_points\n",
    "import random\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "from metrics_utils import * \n",
    "from infer_utils import * \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f901b36",
   "metadata": {},
   "source": [
    "## Init the Valid Input Point Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68193bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 1120 \n",
    "PATCH_DIM = IMAGE_SIZE //14\n",
    "IDX = 3 #id of the object in the category\n",
    "split_train = r\"Objects_train/\"\n",
    "split_test = r\"Objects_test/\"\n",
    "\n",
    "args_infer = {\n",
    "    \"model_path\" : \"runs/objects_10_single.ckpt\",\n",
    "    \"device\" : \"cuda\",\n",
    "    \"img_size\" : IMAGE_SIZE,\n",
    "    \"rgb\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99cb3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args_infer[\"device\"]) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "image_transform = get_transform()\n",
    "inv_transform = get_inv_transform()\n",
    "#model = BCEGraspTransformer(img_size=args_train['img_size'],int_dim=256,output_dim=128,input_cls=1)\n",
    "model = BCEGraspTransformer(img_size=IMAGE_SIZE,int_dim=256,output_dim=128,input_cls=1)\n",
    "dataset_train = JacquardSamples(dataset_root= split_train ,image_transform=image_transform, num_targets=5, overfit=False,\n",
    "                              img_size=args_infer[\"img_size\"], idx=4)\n",
    "dataset = JacquardSamples(dataset_root= split_test ,image_transform=image_transform, num_targets=5, overfit=False,\n",
    "                              img_size=args_infer[\"img_size\"], idx=7)\n",
    "model.load_state_dict(torch.load(args_infer[\"model_path\"]))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88dc32a",
   "metadata": {},
   "source": [
    "## Init the Second Point UNet Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63f5c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (inc): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): Up(\n",
       "    (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): Up(\n",
       "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): Up(\n",
       "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4): Up(\n",
       "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): OutConv(\n",
       "    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unet import UNet\n",
    "\n",
    "unet_path = 'runs/bottle_1_double_unet.ckpt'\n",
    "\n",
    "n_channels = 2 \n",
    "if args_infer[\"rgb\"]:\n",
    "    n_channels += 3 \n",
    "unet = UNet(n_channels=n_channels,n_classes=1)\n",
    "unet.load_state_dict(torch.load(unet_path)['weights'])\n",
    "unet = unet.cuda()\n",
    "unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11019af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cedric/ZeroShot-Grasping/MLP-approach/dataset_jacquard_samples.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_dict['points_grasp'] = torch.tensor(points_grasps)\n",
      "/home/cedric/ZeroShot-Grasping/MLP-approach/dataset_jacquard_samples.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_dict['raw'] = torch.tensor(gknet_labels)\n",
      "/tmp/ipykernel_61189/306462410.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_points = torch.tensor(data[\"points_grasp\"]//14)\n"
     ]
    }
   ],
   "source": [
    "constrain_mode = True\n",
    "TOPK = True\n",
    "topk_grasps = 1\n",
    "topk_input = 5\n",
    "\n",
    "max_dist =0\n",
    "min_dist = 999999\n",
    "for i in range(len(dataset_train)):\n",
    "    data = dataset_train[i]\n",
    "    img = data[\"img\"].to(device)\n",
    "    img = torch.permute(img, (0, 2, 1))\n",
    "    mask = data[\"mask\"].sum().sqrt()\n",
    "    all_points = torch.tensor(data[\"points_grasp\"]//14)\n",
    "    dif = (all_points[:, 0, :] - all_points[:, 1, :]).type(torch.float32).norm(p=2, dim=1)\n",
    "    dif_n = (dif/mask).unsqueeze(1)\n",
    "    if max_dist < dif_n.max():\n",
    "        max_dist = dif_n.max()\n",
    "    if min_dist > dif_n.min() and dif_n.min()>0:\n",
    "        min_dist = dif_n.min()\n",
    "if min_dist < 0.01:\n",
    "    min_dist = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bf229",
   "metadata": {},
   "source": [
    "### Loop over the data and get the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482f227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7964 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/7964 [00:00<1:12:54,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Total accuracy is 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/7964 [00:01<52:19,  2.54it/s]  /home/cedric/anaconda3/envs/4D-humans/lib/python3.10/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n",
      "  return lib.intersection(a, b, **kwargs)\n",
      "  1%|▏         | 101/7964 [00:43<57:16,  2.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100\n",
      "Total accuracy is 61.386138613861384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 201/7964 [01:24<56:14,  2.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200\n",
      "Total accuracy is 65.67164179104478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 284/7964 [02:00<54:30,  2.35it/s]  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (0) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [3, 0, 4].  Tensor sizes: [3, 4, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m start_vis \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     95\u001b[0m \u001b[39m#try : \u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m cur_accuracy \u001b[39m=\u001b[39m vis_preds_with_metrics(num_grasps, items, org_image, grasp,datas[\u001b[39m'\u001b[39;49m\u001b[39mheights\u001b[39;49m\u001b[39m'\u001b[39;49m], args_infer,preds_cp,topks\u001b[39m=\u001b[39;49mtopk_grasps,vis\u001b[39m=\u001b[39;49mVIS)\n\u001b[1;32m     97\u001b[0m accuracies\u001b[39m.\u001b[39mappend(cur_accuracy) \n\u001b[1;32m     98\u001b[0m \u001b[39m#except : \u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m#    print(\"eval issue\")\u001b[39;00m\n",
      "File \u001b[0;32m~/ZeroShot-Grasping/MLP-approach/infer_utils.py:452\u001b[0m, in \u001b[0;36mvis_preds_with_metrics\u001b[0;34m(num_grasps, items, org_image, grasp, heights, args_infer, preds_cp, topks, vis)\u001b[0m\n\u001b[1;32m    450\u001b[0m     new_y \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(pt[\u001b[39m1\u001b[39m])\n\u001b[1;32m    451\u001b[0m     boarder \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m--> 452\u001b[0m     origin_point[:,new_x \u001b[39m-\u001b[39;49m boarder : new_x \u001b[39m+\u001b[39;49m boarder , new_y \u001b[39m-\u001b[39;49m boarder: new_y \u001b[39m+\u001b[39;49m boarder] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m3\u001b[39m,boarder\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m,boarder\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)) \n\u001b[1;32m    453\u001b[0m     origin_point[\u001b[39m2\u001b[39m,new_x \u001b[39m-\u001b[39m boarder : new_x \u001b[39m+\u001b[39m boarder , new_y \u001b[39m-\u001b[39m boarder: new_y \u001b[39m+\u001b[39m boarder] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((boarder\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m,boarder\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)) \n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m correct \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m :\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (0) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [3, 0, 4].  Tensor sizes: [3, 4, 4]"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "\n",
    "accuracies = []\n",
    "print_every = 100\n",
    "VIS = False\n",
    "print_time = False\n",
    "for test_idx in tqdm(range(len(dataset))): \n",
    "    #test_idx = 13\n",
    "    start = time.time()\n",
    "    data = dataset[test_idx]\n",
    "    img = data[\"img\"].to(device)\n",
    "    resized_img = data['resized_img'].to(device)\n",
    "    mask = data[\"mask\"].to(device)\n",
    "    resized_mask = data['resized_mask'].to(device)\n",
    "    img = torch.permute(img, (0, 2, 1))\n",
    "    grasp = data[\"points_grasp\"]//14\n",
    "    normal_grasp = grasp.clone()\n",
    "    grasp_inv = torch.cat([grasp[:,1,:].unsqueeze(1), grasp[:,0,:].unsqueeze(1)], dim=1)\n",
    "    grasp = torch.cat([grasp, grasp_inv], dim=0)\n",
    "    #get features from dino\n",
    "    \n",
    "    features,_ = model.forward_dino_features(img.unsqueeze(0))\n",
    "    features = features.squeeze().reshape(PATCH_DIM, PATCH_DIM, 768)\n",
    "\n",
    "    #plt.figure(figsize=(16,16))\n",
    "    org_image = torch.permute(inv_transform(img), (1, 2, 0)).cpu().numpy()\n",
    "    end = time.time() - start\n",
    "    #plt.imshow(org_image)\n",
    "\n",
    "\n",
    "    ## get valid points\n",
    "    all_points = torch.tensor([[i,j] for j in range(PATCH_DIM) for i in range(PATCH_DIM)]).unsqueeze(1).to(device)\n",
    "\n",
    "    start_valid_pts = time.time()\n",
    "    preds, preds_patches = get_valid_points(all_points, features, model, device,PATCH_DIM=PATCH_DIM)\n",
    "    end_valid_pts = time.time() - start_valid_pts \n",
    "    \n",
    "    start_topk = time.time()\n",
    "    preds_cp, pts, valid_pts_pred = get_topk_valid_points(preds,preds_patches, mask, topk_num=topk_input,TOP_K=TOPK)\n",
    "    end_topk = time.time() - start_topk\n",
    "    \n",
    "    #get second point information using the unet inference \n",
    "    start_second_point = time.time()\n",
    "    unet_preds, conf_vals = get_unet_preds(unet,valid_pts_pred,resized_mask,resized_img,args_infer)\n",
    "    #need to add it to items array here \n",
    "    items = []\n",
    "    #import pdb; pdb.set_trace()\n",
    "    for i in range(len(unet_preds)):\n",
    "        item = {}\n",
    "        origin_point = np.zeros((3, args_infer[\"img_size\"]//14, args_infer[\"img_size\"]//14))\n",
    "        try : \n",
    "            origin_point[:, valid_pts_pred[i][0][0], valid_pts_pred[i][0][1]] = [0, 1, 0]\n",
    "        except : \n",
    "            origin_point[:, int(valid_pts_pred[i][0][0].item()), int(valid_pts_pred[i][0][1].item())] = [0, 1, 0]\n",
    "        \n",
    "        top_x = int(unet_preds[i][0].item()) \n",
    "        top_y = int(unet_preds[i][1].item()) \n",
    "        if constrain_mode == True : \n",
    "            origin_point[:, top_x, top_y] = [0, 1, 1]\n",
    "        origin_point = torch.nn.functional.interpolate(torch.tensor(origin_point).unsqueeze(0), (args_infer[\"img_size\"], args_infer[\"img_size\"]), mode=\"nearest\").squeeze()\n",
    "        \n",
    "    \n",
    "        item['origin_point'] = origin_point\n",
    "        item['preds'] = preds\n",
    "        item['single_point'] = valid_pts_pred[i][0] * 14 + 7\n",
    "        item['pred_point'] = unet_preds[i] * 14 + 7\n",
    "        item['conf'] = conf_vals[i].item()\n",
    "        items.append(item)\n",
    "        #items.append([valid_pts_pred[i],unet_preds[i]])\n",
    "    \n",
    "    end_second_point = time.time() - start_second_point\n",
    "    num_grasps = valid_pts_pred.shape[0]\n",
    "    #objs = 1\n",
    "\n",
    "\n",
    "    datas = {}\n",
    "    datas['mask'] = mask\n",
    "    datas['valid_pts_pred'] = valid_pts_pred\n",
    "    datas['min_dist'] = min_dist\n",
    "    datas['max_dist'] = max_dist\n",
    "    datas['heights'] = data['height'] \n",
    "    datas['args_infer'] = args_infer\n",
    "    datas['mask_n'] = data[\"mask\"]\n",
    "    datas['device'] = device\n",
    "    datas['features'] = features\n",
    "    #data['model_single'] = model_single\n",
    "\n",
    "    ##currently only support constraint mode == false\n",
    "\n",
    "        \n",
    "\n",
    "    start_vis = time.time()\n",
    "    try : \n",
    "        cur_accuracy = vis_preds_with_metrics(num_grasps, items, org_image, grasp,datas['heights'], args_infer,preds_cp,topks=topk_grasps,vis=VIS)\n",
    "        accuracies.append(cur_accuracy) \n",
    "    except : \n",
    "        print(\"eval issue\")\n",
    "    end_vis = time.time() - start_vis\n",
    "        \n",
    "    #except : \n",
    "    \n",
    "    if print_time == True : \n",
    "        all_runtime = float(end + end_valid_pts + end_topk + end_second_point +  end_vis) \n",
    "        data = [['Dinov2',round(end / all_runtime,4) * 100, round(end,4) * 1000], \n",
    "                ['Valid Points', round(end_valid_pts/ all_runtime,2) * 100, round(end_valid_pts,4) * 1000],    \n",
    "                ['Topk', round(end_topk/ all_runtime,4) * 100, round(end_topk,4) * 1000],\n",
    "                ['Second Point', round(end_second_point/ all_runtime,4) * 100, round(end_second_point,4) * 1000],\n",
    "                ['Visualization',round( end_vis/ all_runtime,4) * 100,round(end_vis,4) * 1000],       \n",
    "        ]\n",
    "        \n",
    "        headers = ['Module Type', 'Runtime Percentage', 'Time in ms']\n",
    "        table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "        print(table)\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    #except : \n",
    "    #    print('issue with eval')\n",
    "    #    #accuracies.append(0)\n",
    "    \n",
    "    #print(\"idx\", test_idx)\n",
    "\n",
    "    if test_idx % print_every == 0: \n",
    "        print(\"Iteration\", test_idx)\n",
    "        print('Total accuracy is', sum(accuracies)/len(accuracies))\n",
    "\n",
    "        \n",
    "print('Total accuracy is', sum(accuracies)/len(accuracies))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
